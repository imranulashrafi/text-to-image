{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-to-image+2nd-Stage-noselfatt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cruXOtgObofn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8f4f7728-0e08-4f78-b953-02ab34aeba57"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install tensorboardX\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/76/89dd44458eb976347e5a6e75eb79fecf8facd46c1ce259bad54e0044ea35/tensorboardX-1.6-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (40.9.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF2Ew6ogbofu"
      },
      "source": [
        "##########################################################\n",
        "###DATASET CLASS\n",
        "##########################################################\n",
        "\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import os\n",
        "import os.path\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, data_dir, split = 'train', embedding_type = 'cnn-rnn',\n",
        "                 imsize = 256, transform = None, target_transform = None):\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.imsize = imsize\n",
        "        self.data = []\n",
        "        self.data_dir = data_dir\n",
        "        self.bbox = self.load_bbox()\n",
        "        split_dir = os.path.join(data_dir, split)\n",
        "\n",
        "        self.filenames = self.load_filenames(split_dir)\n",
        "        self.embeddings = self.load_embedding(split_dir, embedding_type)\n",
        "        self.class_id = self.load_class_id(split_dir, len(self.filenames))\n",
        "    \n",
        "    def get_img(self, img_path, bbox):\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        width, height = img.size\n",
        "        if bbox is not None:\n",
        "            R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
        "            center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
        "            center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
        "            y1 = np.maximum(0, center_y - R)\n",
        "            y2 = np.minimum(height, center_y + R)\n",
        "            x1 = np.maximum(0, center_x - R)\n",
        "            x2 = np.minimum(width, center_x + R)\n",
        "            img = img.crop([x1, y1, x2, y2])\n",
        "        load_size = int(self.imsize * 76 / 64)\n",
        "        img = img.resize((load_size, load_size), PIL.Image.BILINEAR)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "    \n",
        "    def load_bbox(self):\n",
        "        data_dir = self.data_dir\n",
        "        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n",
        "        df_bounding_boxes = pd.read_csv(bbox_path,\n",
        "                                        delim_whitespace=True,\n",
        "                                        header=None).astype(int)\n",
        "        \n",
        "        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n",
        "        df_filenames = \\\n",
        "            pd.read_csv(filepath, delim_whitespace=True, header=None)\n",
        "        filenames = df_filenames[1].tolist()\n",
        "        #print('Total filenames: ', len(filenames), filenames[0])\n",
        "        \n",
        "        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n",
        "        numImgs = len(filenames)\n",
        "        for i in range(0, numImgs):\n",
        "            # bbox = [x-left, y-top, width, height]\n",
        "            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n",
        "\n",
        "            key = filenames[i][:-4]\n",
        "            filename_bbox[key] = bbox\n",
        "        \n",
        "        return filename_bbox \n",
        "    \n",
        "    def load_all_captions(self):\n",
        "        caption_dict = {}\n",
        "        for key in self.filenames:\n",
        "            caption_name = '%s/text/%s.txt' % (self.data_dir, key)\n",
        "            captions = self.load_captions(caption_name)\n",
        "            caption_dict[key] = captions\n",
        "        return caption_dict\n",
        "\n",
        "    def load_captions(self, caption_name):\n",
        "        cap_path = caption_name\n",
        "        with open(cap_path, \"r\") as f:\n",
        "            captions = f.read().decode('utf8').split('\\n')\n",
        "        captions = [cap.replace(\"\\ufffd\\ufffd\", \" \")\n",
        "                    for cap in captions if len(cap) > 0]\n",
        "        return captions\n",
        "\n",
        "    def load_embedding(self, data_dir, embedding_type):\n",
        "        if embedding_type == 'cnn-rnn':\n",
        "            embedding_filename = '/char-CNN-RNN-embeddings.pickle'\n",
        "        elif embedding_type == 'cnn-gru':\n",
        "            embedding_filename = '/char-CNN-GRU-embeddings.pickle'\n",
        "        elif embedding_type == 'skip-thought':\n",
        "            embedding_filename = '/skip-thought-embeddings.pickle'\n",
        "\n",
        "        with open(data_dir + embedding_filename, 'rb') as f:\n",
        "            embeddings = pickle.load(f, encoding = 'latin1')\n",
        "            embeddings = np.array(embeddings)\n",
        "            # embedding_shape = [embeddings.shape[-1]]\n",
        "            #print('embeddings: ', embeddings.shape)\n",
        "        return embeddings\n",
        "\n",
        "    def load_class_id(self, data_dir, total_num):\n",
        "        if os.path.isfile(data_dir + '/class_info.pickle'):\n",
        "            with open(data_dir + '/class_info.pickle', 'rb') as f:\n",
        "                class_id = pickle.load(f, encoding = 'latin1')\n",
        "        else:\n",
        "            class_id = np.arange(total_num)\n",
        "        return class_id\n",
        "\n",
        "    def load_filenames(self, data_dir):\n",
        "        filepath = os.path.join(data_dir, 'filenames.pickle')\n",
        "        with open(filepath, 'rb') as f:\n",
        "            filenames = pickle.load(f)\n",
        "        #print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n",
        "        return filenames\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        key = self.filenames[index]\n",
        "        # cls_id = self.class_id[index]\n",
        "        #\n",
        "        if self.bbox is not None:\n",
        "            bbox = self.bbox[key]\n",
        "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
        "        else:\n",
        "            bbox = None\n",
        "            data_dir = self.data_dir\n",
        "\n",
        "        # captions = self.captions[key]\n",
        "        embeddings = self.embeddings[index, :, :]\n",
        "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
        "        img = self.get_img(img_name, bbox)\n",
        "\n",
        "        embedding_ix = random.randint(0, embeddings.shape[0]-1)\n",
        "        embedding = embeddings[embedding_ix, :]\n",
        "        if self.target_transform is not None:\n",
        "            embedding = self.target_transform(embedding)\n",
        "        return img, embedding\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AffXDTj0bof7"
      },
      "source": [
        "##########################################################\n",
        "###SELF ATTENTION CLASS\n",
        "##########################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Self_Attn(nn.Module):\n",
        "    \"\"\" Self attention Layer\"\"\"\n",
        "    def __init__(self,in_dim,activation):\n",
        "        super(Self_Attn,self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.activation = activation\n",
        "\n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax  = nn.Softmax(dim=-1) #\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X W X H)\n",
        "            returns :\n",
        "                out : self attention value + input feature\n",
        "                attention: B X N X N (N is Width*Height)\n",
        "        \"\"\"\n",
        "        m_batchsize,C,width ,height = x.size()\n",
        "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
        "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
        "        attention = self.softmax(energy) # BX (N) X (N)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
        "\n",
        "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
        "        out = out.view(m_batchsize,C,width,height)\n",
        "\n",
        "        out = self.gamma*out + x\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW-NN6TJbofy"
      },
      "source": [
        "##########################################################\n",
        "###MODEL CLASS\n",
        "##########################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "#from self_att import self_att\n",
        "#import Self_Attn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "def conv3x3(in_shape, out_shape, stride=1):\n",
        "    return nn.Conv2d(in_shape, out_shape, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "def conv5x5(in_shape, out_shape, stride=1):\n",
        "    return nn.Conv2d(in_shape, out_shape, kernel_size=5, stride=stride,\n",
        "                     padding=2, bias=False)\n",
        "\n",
        "def upResolution(in_shape, out_shape):\n",
        "    val = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            spectral_norm(conv5x5(in_shape, out_shape)),\n",
        "            nn.BatchNorm2d(out_shape),\n",
        "            nn.ReLU(inplace = True)\n",
        "            )\n",
        "    return val\n",
        "  \n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            spectral_norm(conv5x5(in_channels, in_channels)),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(True),\n",
        "            spectral_norm(conv5x5(in_channels, in_channels)),\n",
        "            nn.BatchNorm2d(in_channels))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.block(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class generator1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(generator1, self).__init__()\n",
        "        self.in_dim = 128 *8\n",
        "        self.cond_dim = 128\n",
        "        self.noise_dim = 100\n",
        "        self.doSeq()\n",
        "        #self.attn = Self_Attn(8, 'relu')\n",
        "\n",
        "    def doSeq(self):\n",
        "        inputs = self.noise_dim + self.cond_dim\n",
        "        #print(inputs)\n",
        "        dim = 128\n",
        "        self.ca_net = CA_NET()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(inputs, dim *4*4, bias = False),\n",
        "                nn.BatchNorm1d(dim*4*4),\n",
        "                nn.ReLU(inplace = True)\n",
        "                )\n",
        "\n",
        "        self.up1 = upResolution(dim , dim//2)\n",
        "        self.up2 = upResolution(dim//2, dim//4)\n",
        "        self.up3 = upResolution(dim//4, dim//8)\n",
        "        self.up4 = upResolution(dim//8, dim//16)\n",
        "\n",
        "        self.img = nn.Sequential(\n",
        "                conv5x5(dim//16, 3),\n",
        "                nn.Tanh()\n",
        "                )\n",
        "\n",
        "    def forward(self, text_embd, noise):\n",
        "        #Check shape\n",
        "        c_code, mu, logvar = self.ca_net(text_embd)\n",
        "        codes = torch.cat((noise, c_code), 1)\n",
        "        #codes = torch.cat((noise, text_embd), 1)\n",
        "        #print(\"Noise+Embed: {}\".format(codes.shape))\n",
        "        #Check shape\n",
        "        in_codes = self.fc(codes)\n",
        "        #print(\"After FC1: {}\".format(in_codes.shape))\n",
        "\n",
        "        #Check shape\n",
        "        in_codes = in_codes.view(-1, 128, 4, 4)\n",
        "        #print(\"Change shape: {}\".format(in_codes.shape))\n",
        "        in_codes = self.up1(in_codes)\n",
        "        #print(\"Upsample 1: {}\".format(in_codes.shape))\n",
        "        in_codes = self.up2(in_codes)\n",
        "        #in_codes = self.attn(in_codes)\n",
        "        #print(\"Upsample 2: {}\".format(in_codes.shape))\n",
        "        in_codes = self.up3(in_codes)\n",
        "        #print(\"Upsample 3: {}\".format(in_codes.shape))\n",
        "        in_codes = self.up4(in_codes)\n",
        "        #print(\"Upsample 4: {}\".format(in_codes.shape))\n",
        "\n",
        "        #in_codes = self.attn(in_codes)\n",
        "        #in_codes = self.attn(in_codes)\n",
        "        #print(in_codes)\n",
        "        #print(in_codes.shape)\n",
        "\n",
        "        fakeimg = self.img(in_codes)\n",
        "        return None, fakeimg, mu, logvar\n",
        "\n",
        "class discriminator1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(discriminator1, self).__init__()\n",
        "        self.in_dim = 64\n",
        "        self.cond_dim = 128\n",
        "        self.doSeq()\n",
        "        #self.attn = Self_Attn(512, 'relu')\n",
        "\n",
        "    def doSeq(self):\n",
        "        dim = self.in_dim\n",
        "        #c_dim = self.cond_dim\n",
        "        self.enc_img = nn.Sequential(\n",
        "                spectral_norm(nn.Conv2d(in_channels = 3, out_channels = dim, kernel_size= 4,\n",
        "                          stride = 2, padding=1, bias=False)),\n",
        "                nn.LeakyReLU(0.2, inplace = True),\n",
        "\n",
        "                spectral_norm(nn.Conv2d(in_channels = dim, out_channels = dim*2, kernel_size= 4,\n",
        "                          stride = 2, padding=1, bias=False)),\n",
        "                #nn.BatchNorm2d(dim*2),\n",
        "                nn.LeakyReLU(0.2, inplace = True),\n",
        "\n",
        "                spectral_norm(nn.Conv2d(in_channels = dim*2, out_channels = dim*4, kernel_size= 4,\n",
        "                          stride = 2, padding=1, bias=False)),\n",
        "                #nn.BatchNorm2d(dim*4),\n",
        "                nn.LeakyReLU(0.2, inplace = True),\n",
        "\n",
        "                spectral_norm(nn.Conv2d(in_channels = dim*4, out_channels = dim*8, kernel_size= 4,\n",
        "                          stride = 2, padding=1, bias=False)),\n",
        "                #nn.BatchNorm2d(dim*8),\n",
        "                nn.LeakyReLU(0.2, inplace = True),\n",
        "                )\n",
        "        self.get_cond_logits = getLogits(64,128)\n",
        "\n",
        "    def forward(self, image):\n",
        "        img_embd = self.enc_img(image)\n",
        "        #img_embd = self.attn(img_embd)\n",
        "        #img_embd = self.attn(img_embd)\n",
        "        return img_embd\n",
        "      \n",
        "class generator2(nn.Module):\n",
        "    def __init__(self, generator1):\n",
        "        super(generator2, self).__init__()\n",
        "        self.in_dim = 128\n",
        "        self.condition_dim = 128\n",
        "        self.noise_dim = 100\n",
        "        self.generator1 = generator1\n",
        "        # fix parameters of stageI GAN\n",
        "        for param in self.generator1.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.attn = Self_Attn(8, 'relu')\n",
        "        self.doSeq()\n",
        "\n",
        "    def _make_layer(self, block, channel_num):\n",
        "        layers = []\n",
        "        for i in range(4):\n",
        "            layers.append(block(channel_num))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def doSeq(self):\n",
        "        dim = self.in_dim\n",
        "\n",
        "        self.ca_net = CA_NET()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            spectral_norm(conv5x5(3, dim)),\n",
        "            nn.ReLU(True),\n",
        "            spectral_norm(nn.Conv2d(in_channels = dim, out_channels = dim * 2, kernel_size = 4, \n",
        "                      stride = 2,padding = 1, bias=False)),\n",
        "            nn.BatchNorm2d(dim * 2),\n",
        "            nn.ReLU(True),\n",
        "            spectral_norm(nn.Conv2d(in_channels = dim * 2, out_channels = dim * 4,kernel_size= 4, \n",
        "                      stride = 2, padding = 1, bias=False)),\n",
        "            nn.BatchNorm2d(dim * 4),\n",
        "            nn.ReLU(True))\n",
        "        self.hr_joint = nn.Sequential(\n",
        "            spectral_norm(conv5x5(self.condition_dim + dim * 4, dim * 4)),\n",
        "            nn.BatchNorm2d(dim * 4),\n",
        "            nn.ReLU(True))\n",
        "        \n",
        "        self.residual = self._make_layer(ResBlock, dim * 4)\n",
        "        \n",
        "        self.up1 = upResolution(dim * 4, dim * 2)\n",
        "        self.up2 = upResolution(dim * 2, dim)\n",
        "        self.up3 = upResolution(dim, dim // 2)\n",
        "        self.up4 = upResolution(dim // 2, dim // 4)\n",
        "        self.img = nn.Sequential(\n",
        "            spectral_norm(conv5x5(dim // 4, 3)),\n",
        "            nn.Tanh())\n",
        "        self.rsz = nn.Sequential(\n",
        "            spectral_norm(conv5x5(3, dim//16)),\n",
        "            nn.Tanh())\n",
        "        self.rszs = nn.Sequential(\n",
        "            spectral_norm(conv5x5(dim//16, 3)),\n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, text_embedding, noise):\n",
        "        _, stage1_img, _, _ = self.generator1(text_embedding, noise)\n",
        "        stage1_img = self.rsz(stage1_img)\n",
        "        stage1_img = self.attn(stage1_img)\n",
        "        stage1_img = self.rszs(stage1_img)\n",
        "        \n",
        "        stage1_img = stage1_img.detach()\n",
        "        encoded_img = self.encoder(stage1_img)\n",
        "        #print(encoded_img.shape)\n",
        "        #\n",
        "        #encoded_img = self.attn(encoded_img)\n",
        "        #\n",
        "\n",
        "        c_code, mu, logvar = self.ca_net(text_embedding)\n",
        "        c_code = c_code.view(-1, self.in_dim, 1, 1)\n",
        "        c_code = c_code.repeat(1, 1, 16, 16)\n",
        "        i_c_code = torch.cat([encoded_img, c_code], 1)\n",
        "        h_code = self.hr_joint(i_c_code)\n",
        "        h_code = self.residual(h_code)\n",
        "\n",
        "        h_code = self.up1(h_code)\n",
        "        h_code = self.up2(h_code)\n",
        "        h_code = self.up3(h_code)\n",
        "        h_code = self.up4(h_code)\n",
        "\n",
        "        fake_img = self.img(h_code)\n",
        "        #print(\"Fake img shape: {}\", fake_img.shape)\n",
        "        return stage1_img, fake_img, mu, logvar\n",
        "      \n",
        "class discriminator2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(discriminator2, self).__init__()\n",
        "        self.df_dim = 64\n",
        "        self.ef_dim = 128\n",
        "        self.define_module()\n",
        "        self.attn = Self_Attn(512, 'relu')\n",
        "\n",
        "    def define_module(self):\n",
        "        ndf, nef = self.df_dim, self.ef_dim\n",
        "        self.encode_img = nn.Sequential(\n",
        "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),  # 128 * 128 * ndf\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),  # 64 * 64 * ndf * 2\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),  # 32 * 32 * ndf * 4\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),  # 16 * 16 * ndf * 8\n",
        "            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 16),\n",
        "            nn.LeakyReLU(0.2, inplace=True),  # 8 * 8 * ndf * 16\n",
        "            nn.Conv2d(ndf * 16, ndf * 32, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),  # 4 * 4 * ndf * 32\n",
        "            conv5x5(ndf * 32, ndf * 16),\n",
        "            nn.BatchNorm2d(ndf * 16),\n",
        "            nn.LeakyReLU(0.2, inplace=True),   # 4 * 4 * ndf * 16\n",
        "            spectral_norm(conv5x5(ndf * 16, ndf * 8)),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)   # 4 * 4 * ndf * 8\n",
        "        )\n",
        "\n",
        "        self.get_cond_logits = getLogits(ndf, nef, bcondition=True)\n",
        "        self.get_uncond_logits = getLogits(ndf, nef, bcondition=False)\n",
        "\n",
        "    def forward(self, image):\n",
        "        img_embedding = self.encode_img(image)\n",
        "        img_embedding = self.attn(img_embedding)\n",
        "\n",
        "        return img_embedding\n",
        "\n",
        "\n",
        "class getLogits(nn.Module):\n",
        "    def __init__(self, dim, cond, bcondition=True):\n",
        "        super(getLogits, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.cond = cond\n",
        "        self.bcondition = bcondition\n",
        "        if bcondition:\n",
        "            self.logits = nn.Sequential(\n",
        "                    conv3x3(dim*8 + cond, dim*8),\n",
        "                    nn.BatchNorm2d(dim*8),\n",
        "                    nn.Conv2d(in_channels = dim*8, out_channels=1, kernel_size=4,\n",
        "                              stride = 4),\n",
        "                    nn.Sigmoid()\n",
        "                    )\n",
        "        else:\n",
        "            self.logits = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels = dim *8, out_channels = 1, kernel_size=4,\n",
        "                    stride=4),\n",
        "                    nn.Sigmoid()\n",
        "                )\n",
        "\n",
        "    def forward(self, codes, c_code = None):\n",
        "        if self.bcondition and c_code is not None:\n",
        "            #print(\"Feature Codes Shape: {}\".format(codes.shape))\n",
        "            #print(\"Condition Code Shape: {}\".format(c_code.shape))\n",
        "            c_code = c_code.view(-1, self.cond, 1,1)\n",
        "            #print(\"After reshape: {}\".format(c_code.shape))\n",
        "            c_code = c_code.repeat(1,1,4,4)\n",
        "            #print(\"repeat: {}\", c_code.shape)\n",
        "            codes = torch.cat((codes, c_code), 1)\n",
        "            #print(\"Codes+Cond. Code: {}\".format(codes.shape))\n",
        "        else:\n",
        "            codes = codes\n",
        "        outputs = self.logits(codes)\n",
        "        return outputs.view(-1)\n",
        "\n",
        "class CA_NET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CA_NET, self).__init__()\n",
        "        self.t_dim = 1024\n",
        "        self.c_dim = 128\n",
        "        self.fc = nn.Linear(self.t_dim, self.c_dim * 2, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def encode(self, text_embedding):\n",
        "        x = self.relu(self.fc(text_embedding))\n",
        "        mu = x[:, :self.c_dim]\n",
        "        logvar = x[:, self.c_dim:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparametrize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        #eps = torch.FloatTensor(std.size()).normal_()\n",
        "        eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
        "        eps = Variable(eps)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def forward(self, text_embedding):\n",
        "        mu, logvar = self.encode(text_embedding)\n",
        "        c_code = self.reparametrize(mu, logvar)\n",
        "        return c_code, mu, logvar\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HT579IMbof2"
      },
      "source": [
        "##########################################################\n",
        "###LOSS CLASS\n",
        "##########################################################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def disc_loss(dis, real_imgs, fake_imgs, real_labels, fake_labels, conditions):\n",
        "    criterion = nn.BCELoss()\n",
        "    batch_size = real_imgs.size(0)\n",
        "    #Check expln\n",
        "    cond = conditions.detach()\n",
        "    #print(\"Conditions Shape: {}\".format(cond.shape))\n",
        "    fake = fake_imgs.detach()\n",
        "    #print(\"Fake imgs Shape: {}\".format(fake.shape))\n",
        "    #print(\"Real imgs shape: {}\", real_imgs.shape)\n",
        "    real_features = dis(real_imgs)\n",
        "    #print(\"Real Features Shape: {}\".format(real_features.shape))\n",
        "    fake_features = dis(fake)\n",
        "    #print(\"Fake Features Shape: {}\".format(fake_features.shape))\n",
        "    #real pair\n",
        "    real_logits = dis.get_cond_logits(real_features, cond)\n",
        "    #print(\"Real Logits Shape: {}\".format(real_logits.shape))\n",
        "    dis_error_real = criterion(real_logits, real_labels)\n",
        "    #wrong pairs\n",
        "    wrong_logits = dis.get_cond_logits(real_features[:(batch_size-1)], cond[1:])\n",
        "    #print(\"Wrong Logits Shape: {}\".format(wrong_logits.shape))\n",
        "    dis_error_wrong = criterion(wrong_logits, fake_labels[1:])\n",
        "    #fake paris\n",
        "    fake_logits = dis.get_cond_logits(fake_features, cond)\n",
        "    #print(\"Fake Logits Shape: {}\".format(fake_logits.shape))\n",
        "    dis_error_fake = criterion(fake_logits, fake_labels)\n",
        "    \n",
        "    if dis.get_uncond_logits is not None:\n",
        "      real_logits = dis.get_uncond_logits(real_features)\n",
        "      fake_logits = dis.get_uncond_logits(fake_features)\n",
        "      uncond_dis_error_real = criterion(real_logits, real_labels)\n",
        "      uncond_dis_error_fake = criterion(fake_logits, fake_labels)\n",
        "      errorz = ((dis_error_real+uncond_dis_error_real)/2. + (dis_error_fake +\n",
        "                 uncond_dis_error_fake + dis_error_wrong)/3.)\n",
        "      dis_error_real = (dis_error_real + uncond_dis_error_real)/2.\n",
        "      dis_error_fake = (dis_error_fake + uncond_dis_error_fake)/2.\n",
        "    else:\n",
        "      errorz = dis_error_real + (dis_error_fake + dis_error_wrong) * 0.5\n",
        "    #print(\"Total Error: {}\".format(errorz))\n",
        "    return errorz, dis_error_real.data.item(), dis_error_wrong.data.item(), dis_error_fake.data.item()\n",
        "\n",
        "def gen_loss(dis, fake_imgs, real_labels, conditions):\n",
        "    criterion = nn.BCELoss()\n",
        "    cond = conditions.detach()\n",
        "    fake_features = dis(fake_imgs)\n",
        "    #fake pairs\n",
        "    fake_logits = dis.get_cond_logits(fake_features, cond)\n",
        "    errorz = criterion(fake_logits, real_labels)\n",
        "    if dis.get_uncond_logits is not None:\n",
        "      fake_logits = dis.get_uncond_logits(fake_features)\n",
        "      uncond_errorz = criterion(fake_logits, real_labels)\n",
        "      errorz = errorz + uncond_errorz\n",
        "    #print('Total Error: {}'.format(errorz))\n",
        "    return errorz\n",
        "\n",
        "def KL_loss(mu, logvar):\n",
        "    # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
        "    KLD = torch.mean(KLD_element).mul_(-0.5)\n",
        "    return KLD\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiEcrngBbof_"
      },
      "source": [
        "##########################################################\n",
        "###UTILS CLASS\n",
        "##########################################################\n",
        "\n",
        "import os\n",
        "import errno\n",
        "import numpy as np\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from torch.nn import init\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.0)\n",
        "            \n",
        "def save_img_results(data_img, fake, epoch, image_dir):\n",
        "    num = 64\n",
        "    fake = fake[0:num]\n",
        "    # data_img is changed to [0,1]\n",
        "    if data_img is not None:\n",
        "        data_img = data_img[0:num]\n",
        "        vutils.save_image(\n",
        "            data_img, '%s/real_samples.png' % image_dir,\n",
        "            normalize=True)\n",
        "        # fake.data is still [-1, 1]\n",
        "        vutils.save_image(\n",
        "            fake.data, '%s/fake_samples_epoch_%03d.png' %\n",
        "            (image_dir, epoch), normalize=True)\n",
        "    else:\n",
        "        vutils.save_image(\n",
        "            fake.data, '%s/lr_fake_samples_epoch_%03d.png' %\n",
        "            (image_dir, epoch), normalize=True)\n",
        "        \n",
        "def save_model(netG, netD, epoch, model_dir):\n",
        "    torch.save(\n",
        "        netG.state_dict(),\n",
        "        '%s/netG_epoch_last.pth' % (model_dir))\n",
        "    torch.save(\n",
        "        netD.state_dict(),\n",
        "        '%s/netD_epoch_last.pth' % (model_dir))\n",
        "    #print('Save G/D models')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXl-QzscbogE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1914faef-b3a0-4563-8095-7f07b471ec17"
      },
      "source": [
        "##########################################################\n",
        "###TRAINER CLASS\n",
        "##########################################################\n",
        "\n",
        "import os\n",
        "#print(!ls)\n",
        "#os.chdir('/content/gdrive/My Drive/Test')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "#import generator, discriminator\n",
        "#import disc_loss, gen_loss, KL_loss\n",
        "from tensorboardX import summary\n",
        "from tensorboardX import FileWriter\n",
        "#import weights_init, save_img_results, save_model\n",
        "\n",
        "#OUTPUS\n",
        "model_dir = '/content/gdrive/My Drive/OUTPUTS/STAGE2WA/model'\n",
        "image_dir = '/content/gdrive/My Drive/OUTPUTS/STAGE2WA/image'\n",
        "log_dir = '/content/gdrive/My Drive/OUTPUTS/STAGE2WA/log'\n",
        "summary_writer = FileWriter(log_dir)\n",
        "\n",
        "#Networks\n",
        "gen1 = generator1()\n",
        "\n",
        "gen = generator2(gen1)\n",
        "gen.apply(weights_init)\n",
        "\n",
        "dis = discriminator2()\n",
        "dis.apply(weights_init)\n",
        "\n",
        "gen.generator1.load_state_dict(torch.load('/content/gdrive/My Drive/OUTPUTS/netG_epoch_280.pth'))\n",
        "gen.load_state_dict(torch.load('/content/gdrive/My Drive/OUTPUTS/STAGE2WA/model/netG_epoch_last.pth'))\n",
        "dis.load_state_dict(torch.load('/content/gdrive/My Drive/OUTPUTS/STAGE2WA/model/netD_epoch_last.pth'))\n",
        "\n",
        "dis.cuda()\n",
        "gen.cuda()\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(256),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "dataset = Dataset('/content/gdrive/My Drive/CUB_200_2011/DATA',\n",
        "                  'train', imsize = 256, transform = image_transform)\n",
        "#print(dataset)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=8, drop_last=True, shuffle=True)\n",
        "#print(dataloader)\n",
        "\n",
        "noise = Variable(torch.FloatTensor(8,100))\n",
        "#print('Noise shape: {}'.format(noise.shape))\n",
        "fixed_noise = Variable(torch.FloatTensor(8, 100).normal_(0,1), volatile = True)\n",
        "real_labels = Variable(torch.FloatTensor(8).fill_(1))\n",
        "fake_labels = Variable(torch.FloatTensor(8).fill_(0))\n",
        "noise = noise.cuda()\n",
        "fixed_noise = fixed_noise.cuda()\n",
        "real_labels = real_labels.cuda()\n",
        "fake_labels = fake_labels.cuda()\n",
        "\n",
        "optim_dis = optim.Adam(dis.parameters(), lr = 0.0001, betas = (0.5, 0.999))\n",
        "optim_gen = optim.Adam(gen.parameters(), lr = 0.0001, betas = (0.5, 0.999))\n",
        "\n",
        "dis_error=[]\n",
        "gen_error=[]\n",
        "count = 0\n",
        "for epoch in range(128,100):\n",
        "    for i,datas in enumerate(dataloader, 0):\n",
        "        real_img_org, txt_embd = datas\n",
        "        #Torch variables\n",
        "        real_img = Variable(real_img_org)\n",
        "        txt_embd = Variable(txt_embd)\n",
        "        real_img = real_img.cuda()\n",
        "        txt_embd = txt_embd.cuda()\n",
        "        #print('Text Embade shape: {}'.format(txt_embd.shape))\n",
        "        noise.data.normal_(0,1)\n",
        "        #print('Noise shape: {}'.format(noise.shape))\n",
        "        #print('GENERATING::')\n",
        "        _, fake_img, mu, logvar = gen(txt_embd, noise)\n",
        "        #print(\"Fake img shape: {}\".format(fake_img.shape))\n",
        "        #update discriminator\n",
        "        #print('DISCRIMINATOR LOSS::')\n",
        "        dis.zero_grad()\n",
        "        errorDz, dis_error_real, dis_error_wrong, dis_error_fake = disc_loss(\n",
        "                dis, real_img, fake_img, real_labels, fake_labels, mu)\n",
        "        if epoch > 25:\n",
        "          errorDz.backward()\n",
        "          optim_dis.step()\n",
        "        else:\n",
        "          if errorDz > 1:\n",
        "            errorDz.backward()\n",
        "            optim_dis.step()\n",
        "        #update generator\n",
        "        #print('GENERATOR LOSS::')\n",
        "        gen.zero_grad()\n",
        "        errorGz = gen_loss(dis, fake_img, real_labels, mu)\n",
        "        kl_loss = KL_loss(mu, logvar)\n",
        "        errorGzT = kl_loss * 2.0 + errorGz\n",
        "        errorGzT.backward()\n",
        "        optim_gen.step()\n",
        "        \n",
        "        count = count+1\n",
        "        '''\n",
        "        if i !=0:\n",
        "          if i%100 == 0:\n",
        "            summary_D = summary.scalar('D_loss', errorDz.item())\n",
        "            summary_D_real = summary.scalar('D_loss_real', dis_error_real)\n",
        "            summary_D_wrong = summary.scalar('D_loss_wrong', dis_error_wrong)\n",
        "            summary_D_fake = summary.scalar('D_loss_fake', dis_error_fake)\n",
        "            summary_G = summary.scalar('G_loss', errorGz.item())\n",
        "\n",
        "            summary_writer.add_summary(summary_D, count)\n",
        "            summary_writer.add_summary(summary_D_real, count)\n",
        "            summary_writer.add_summary(summary_D_wrong, count)\n",
        "            summary_writer.add_summary(summary_D_fake, count)\n",
        "            summary_writer.add_summary(summary_G, count)\n",
        "        \n",
        "            _, fake, _, _ =  gen(txt_embd, fixed_noise)\n",
        "            save_img_results(real_img_org, fake, epoch, image_dir)\n",
        "        \n",
        "            print(\"#############################################################\")\n",
        "            print(\"Epoch: {}\".format(epoch))\n",
        "            print(\"Discriminator Loss: {}\".format(errorDz.item()))\n",
        "            print(\"Generator Loss: {}\".format(errorGz.item()))\n",
        "            print(\"Real Loss: {}\".format(dis_error_real))\n",
        "            print(\"Wrong Loss: {}\".format(dis_error_wrong))\n",
        "            print(\"Fake Loss: {}\".format(dis_error_fake))\n",
        "            print(\"#############################################################\")\n",
        "            \n",
        "            dis_error.append(errorDz.item())\n",
        "            gen_error.append(errorGzT.item())\n",
        "         '''\n",
        "    print(\"#############################################################\")\n",
        "    print(\"Epoch: {}\".format(epoch))\n",
        "    print(\"Discriminator Loss: {}\".format(errorDz.item()))\n",
        "    print(\"Generator Loss: {}\".format(errorGz.item()))\n",
        "    print(\"Real Loss: {}\".format(dis_error_real))\n",
        "    print(\"Wrong Loss: {}\".format(dis_error_wrong))\n",
        "    print(\"Fake Loss: {}\".format(dis_error_fake))\n",
        "    print(\"#############################################################\")\n",
        "    dis_error.append(errorDz.item())\n",
        "    gen_error.append(errorGzT.item())\n",
        "    _, fake, _, _ =  gen(txt_embd, fixed_noise)\n",
        "    save_img_results(real_img_org, fake, epoch, image_dir)\n",
        "    save_model(gen, dis, epoch, model_dir)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHIZH-y2iR-j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}